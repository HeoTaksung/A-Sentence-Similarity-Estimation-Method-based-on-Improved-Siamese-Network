{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_sentence(left_texts, right_texts, vocab_index):\n",
    "    left_int = []\n",
    "    right_int = []\n",
    "\n",
    "    for i in range(len(left_texts)):\n",
    "        left_etc = []\n",
    "        right_etc = []\n",
    "        for j in range(len(left_texts[i])):\n",
    "            left_etc.append(vocab_index[left_texts[i][j]])\n",
    "        for j in range(len(right_texts[i])):\n",
    "            right_etc.append(vocab_index[right_texts[i][j]])\n",
    "        left_int.append(left_etc)\n",
    "        right_int.append(right_etc)\n",
    "        \n",
    "    return left_int, right_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = open('file_name_1', 'r', encoding='utf-8-sig')   # train, cv, test가 모두 있는 파일\n",
    "train = open('file_name_2', 'r', encoding='utf-8-sig')\n",
    "cv = open('file_name_3', 'r', encoding='utf-8-sig')\n",
    "test = open('file_name_4', 'r', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = []\n",
    "\n",
    "for line in All:\n",
    "    line = line.split('\\t')\n",
    "    all_sentence.append(line[0].split())\n",
    "    all_sentence.append(line[1].split())\n",
    "\n",
    "max_len = max([len(i) for i in all_sentence])\n",
    "\n",
    "vocab = set()\n",
    "for line in all_sentence:\n",
    "    for word in line:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)+1\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "vocab_index = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_index[vocab[i]] = len(vocab_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished make embedding index ---\n",
      "Found 12178 word vectors.\n",
      "Created Embedded Matrix: 12178 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embed_model_dir = 'embedding_pretrain_file'\n",
    "\n",
    "embedding_index = dict()\n",
    "\n",
    "with open(embed_model_dir, 'r', encoding='utf-8-sig') as f:\n",
    "    for line in f:\n",
    "        # Exception to first line in word2vec model.\n",
    "        if len(line.split()) == 2:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vectors\n",
    "    f.close()\n",
    "    print('--- Finished make embedding index ---')\n",
    "print('Found %s word vectors.' % len(embedding_index))\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedded_matrix = np.zeros((len(vocab_index)+1, EMBEDDING_DIM))\n",
    "embed_cnt = 0\n",
    "\n",
    "for word, i in vocab_index.items():\n",
    "    embedded_vector = embedding_index.get(word)\n",
    "    if embedded_vector is not None:\n",
    "        embedded_matrix[i] = embedded_vector\n",
    "        embed_cnt += 1\n",
    "\n",
    "print('Created Embedded Matrix: %s word vectors.' % embed_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_layer = layers.Embedding(len(vocab_index)+1, EMBEDDING_DIM, \n",
    "    weights=[embedded_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "train_left_sen = []\n",
    "train_right_sen = []\n",
    "train_label = []\n",
    "\n",
    "for line in train:\n",
    "    line = line.split('\\t')\n",
    "    train_left_sen.append(line[0].strip().split())\n",
    "    train_right_sen.append(line[1].strip().split())\n",
    "    train_label.append(float(line[2].strip()))\n",
    "\n",
    "cv_left_sen = []\n",
    "cv_right_sen = []\n",
    "cv_label = []\n",
    "\n",
    "for line in cv:\n",
    "    line = line.split('\\t')\n",
    "    cv_left_sen.append(line[0].strip().split())\n",
    "    cv_right_sen.append(line[1].strip().split())\n",
    "    cv_label.append(float(line[2].strip()))\n",
    "\n",
    "test_left_sen = []\n",
    "test_right_sen = []\n",
    "test_label = []\n",
    "\n",
    "for line in test:\n",
    "    line = line.split('\\t')\n",
    "    test_left_sen.append(line[0].strip().split())\n",
    "    test_right_sen.append(line[1].strip().split())\n",
    "    test_label.append(float(line[2].strip()))\n",
    "\n",
    "train_left_int, train_right_int = int_sentence(train_left_sen, train_right_sen, vocab_index)\n",
    "cv_left_int, cv_right_int = int_sentence(cv_left_sen, cv_right_sen, vocab_index)\n",
    "test_left_int, test_right_int = int_sentence(test_left_sen, test_right_sen, vocab_index)\n",
    "\n",
    "train_left = pad_sequences(train_left_int, padding='post', maxlen=max_len)\n",
    "train_right = pad_sequences(train_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "cv_left = pad_sequences(cv_left_int, padding='post', maxlen=max_len)\n",
    "cv_right = pad_sequences(cv_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "test_left = pad_sequences(test_left_int, padding='post', maxlen=max_len)\n",
    "test_right = pad_sequences(test_right_int, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 69)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 69)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 69, 300)      3849300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 69, 50)       70200       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 69, 50)       0           lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 69, 50)       0           lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 69, 50)       3265        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 69, 50)       3265        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 3450)         0           seq_self_attention_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 3450)         0           seq_self_attention_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6900)         0           flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          883328      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            129         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,809,487\n",
      "Trainable params: 960,187\n",
      "Non-trainable params: 3,849,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_input = layers.Input(shape=(max_len,))\n",
    "right_input = layers.Input(shape=(max_len,))\n",
    "\n",
    "left_emd = embedded_layer(left_input)\n",
    "right_emd = embedded_layer(right_input)\n",
    "\n",
    "lstm = layers.LSTM(50, return_sequences=True)\n",
    "\n",
    "left_lstm = lstm(left_emd)\n",
    "right_lstm = lstm(right_emd)\n",
    "\n",
    "left_lstm = layers.Dropout(0.2)(left_lstm)\n",
    "right_lstm = layers.Dropout(0.2)(right_lstm)\n",
    "\n",
    "left_attention = SeqSelfAttention(attention_activation='tanh')(left_lstm)\n",
    "right_attention = SeqSelfAttention(attention_activation='tanh')(right_lstm)\n",
    "\n",
    "left_f = layers.Flatten()(left_attention)\n",
    "right_f = layers.Flatten()(right_attention)\n",
    "\n",
    "concat = layers.concatenate([left_f, right_f], axis=-1)\n",
    "\n",
    "concat = layers.Dense(128, activation='tanh')(concat)\n",
    "\n",
    "output = layers.Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.0001))(concat)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "model = Model(inputs=[left_input, right_input], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8800 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "8800/8800 [==============================] - 28s 3ms/step - loss: 0.2077 - acc: 0.6769 - val_loss: 0.1906 - val_acc: 0.6982\n",
      "Epoch 2/100\n",
      "8800/8800 [==============================] - 26s 3ms/step - loss: 0.1798 - acc: 0.7360 - val_loss: 0.1809 - val_acc: 0.7273\n",
      "Epoch 3/100\n",
      "8800/8800 [==============================] - 26s 3ms/step - loss: 0.1605 - acc: 0.7712 - val_loss: 0.1753 - val_acc: 0.7364\n",
      "Epoch 4/100\n",
      "8800/8800 [==============================] - 26s 3ms/step - loss: 0.1469 - acc: 0.7964 - val_loss: 0.1661 - val_acc: 0.7773\n",
      "Epoch 5/100\n",
      "8800/8800 [==============================] - 27s 3ms/step - loss: 0.1288 - acc: 0.8280 - val_loss: 0.1671 - val_acc: 0.7818\n",
      "Epoch 6/100\n",
      "8800/8800 [==============================] - 25s 3ms/step - loss: 0.1135 - acc: 0.8511 - val_loss: 0.1578 - val_acc: 0.7845\n",
      "Epoch 7/100\n",
      "8800/8800 [==============================] - 27s 3ms/step - loss: 0.0989 - acc: 0.8743 - val_loss: 0.1506 - val_acc: 0.7900\n",
      "Epoch 8/100\n",
      "8800/8800 [==============================] - 27s 3ms/step - loss: 0.0866 - acc: 0.8917 - val_loss: 0.1565 - val_acc: 0.7955\n",
      "Epoch 9/100\n",
      "8800/8800 [==============================] - 27s 3ms/step - loss: 0.0743 - acc: 0.9098 - val_loss: 0.1335 - val_acc: 0.8291\n",
      "Epoch 10/100\n",
      "8800/8800 [==============================] - 25s 3ms/step - loss: 0.0679 - acc: 0.9169 - val_loss: 0.1427 - val_acc: 0.8100\n",
      "Epoch 11/100\n",
      "8800/8800 [==============================] - 26s 3ms/step - loss: 0.0562 - acc: 0.9327 - val_loss: 0.1364 - val_acc: 0.8336\n",
      "Epoch 12/100\n",
      "8800/8800 [==============================] - 27s 3ms/step - loss: 0.0491 - acc: 0.9423 - val_loss: 0.1402 - val_acc: 0.8264\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00012: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1db9263240>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit([train_left, train_right], [train_label], batch_size=64, epochs=100, validation_data=([cv_left, cv_right], [cv_label]), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 4s 4ms/step\n",
      "Accuracy: 0.8445454545454546\n",
      "Loss: 0.12239359594204209\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate([test_left, test_right], [test_label])\n",
    "\n",
    "print('Accuracy: '+str(evaluation[1]))\n",
    "print('Loss: '+str(evaluation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
